{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8276324d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhijit\\documents\\github\\yhack-mini\\notebooks\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neulab/codebert-python\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"neulab/codebert-python\")\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d185adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5309908986091614, 'token': 939, 'token_str': ' i', 'sequence': 'while i < 5:'}\n",
      "{'score': 0.05808789283037186, 'token': 295, 'token_str': ' n', 'sequence': 'while n < 5:'}\n",
      "{'score': 0.057210806757211685, 'token': 3023, 'token_str': ' x', 'sequence': 'while x < 5:'}\n",
      "{'score': 0.05129788815975189, 'token': 449, 'token_str': ' k', 'sequence': 'while k < 5:'}\n",
      "{'score': 0.03887845575809479, 'token': 1236, 'token_str': ' j', 'sequence': 'while j < 5:'}\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(\"while <mask> < 5:\")\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a2cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.875249445438385, 'token': 8, 'token_str': ' and', 'sequence': 'if (x is not None) and (x > 0)'}\n",
      "{'score': 0.017183667048811913, 'token': 50, 'token_str': ' or', 'sequence': 'if (x is not None) or (x > 0)'}\n",
      "{'score': 0.013177888467907906, 'token': 463, 'token_str': 'and', 'sequence': 'if (x is not None)and (x > 0)'}\n",
      "{'score': 0.012697670608758926, 'token': 671, 'token_str': ' return', 'sequence': 'if (x is not None) return (x > 0)'}\n",
      "{'score': 0.010224265046417713, 'token': 48200, 'token_str': ' &&', 'sequence': 'if (x is not None) && (x > 0)'}\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(\"if (x is not None) <mask> (x > 0)\")\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83439bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9669309854507446, 'token': 16, 'token_str': ' is', 'sequence': '<s>if var1 is<mask> None:</s>'}\n",
      "{'score': 0.00770614156499505, 'token': 328, 'token_str': '!', 'sequence': '<s>if var1!<mask> None:</s>'}\n",
      "{'score': 0.002733553759753704, 'token': 28696, 'token_str': ' <', 'sequence': '<s>if var1 <<mask> None:</s>'}\n",
      "{'score': 0.002088442211970687, 'token': 50118, 'token_str': '\\n', 'sequence': '<s>if var1\\n<mask> None:</s>'}\n",
      "{'score': 0.0018904786556959152, 'token': 35, 'token_str': ':', 'sequence': '<s>if var1:<mask> None:</s>'}\n",
      "\n",
      "{'score': 0.9925784468650818, 'token': 45, 'token_str': ' not', 'sequence': '<s>if var1<mask> not None:</s>'}\n",
      "{'score': 0.001338448142632842, 'token': 5214, 'token_str': '=', 'sequence': '<s>if var1<mask>= None:</s>'}\n",
      "{'score': 0.0013240614207461476, 'token': 16, 'token_str': ' is', 'sequence': '<s>if var1<mask> is None:</s>'}\n",
      "{'score': 0.0009433833765797317, 'token': 49333, 'token_str': '!=', 'sequence': '<s>if var1<mask>!= None:</s>'}\n",
      "{'score': 0.0006681906525045633, 'token': 45994, 'token_str': ' ==', 'sequence': '<s>if var1<mask> == None:</s>'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(\"if var1 <mask> <mask> None:\")\n",
    "for output in outputs:\n",
    "    for sub_output in output:\n",
    "        print(sub_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27dd0587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.1390775889158249, 'token': 17265, 'token_str': 'print', 'sequence': 'print ( x )'}\n",
      "{'score': 0.09198562055826187, 'token': 5780, 'token_str': ' print', 'sequence': ' print ( x )'}\n",
      "{'score': 0.03168381378054619, 'token': 41975, 'token_str': 'import', 'sequence': 'import ( x )'}\n",
      "{'score': 0.01466455589979887, 'token': 1423, 'token_str': ' y', 'sequence': ' y ( x )'}\n",
      "{'score': 0.013982338830828667, 'token': 37131, 'token_str': ' eval', 'sequence': ' eval ( x )'}\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(\"<mask> ( x )\")\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f08ae37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print ( \"Hello World\" ]\n",
      "CHANGE { print } to { [ } ( 5.5458804993758575 )\n",
      "CHANGE { ( } to { [ } ( 6651.068348958294 )\n",
      "CHANGE { \"Hello World\" } to { [ } ( 5.563487649735294 )\n",
      "CHANGE { ] } to { ) } ( 26.31283005431899 )\n",
      "\n",
      "[(('(', 5, 6), '[')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tokenize import generate_tokens\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "def merge_outputs(outputs):\n",
    "    probs = defaultdict(float)\n",
    "    for output in outputs:\n",
    "        probs[output[\"token_str\"].strip()] += output[\"score\"]\n",
    "    return probs\n",
    "\n",
    "def tokenize(lines, line):\n",
    "    N = len(lines)\n",
    "    tokens = list(generate_tokens(lambda L=iter(lines): next(L)))\n",
    "    filtered_tokens = defaultdict(list)\n",
    "    for token in tokens:\n",
    "        filtered_tokens[token.start[0] - 1].append(token)\n",
    "    line_lengths = [filtered_tokens[i][-1].end[1] for i in range(N)]\n",
    "    cumulative_lengths = [sum(line_lengths[:i]) for i in range(N)]\n",
    "    \n",
    "    curr_tokens = filtered_tokens[line]\n",
    "    curr_token_strings = [x.string for x in curr_tokens]\n",
    "    line_offset = cumulative_lengths[line]\n",
    "    \n",
    "    print(\" \".join(curr_token_strings).strip())\n",
    "    \n",
    "    return curr_tokens, curr_token_strings, line_offset\n",
    "\n",
    "\n",
    "def get_best_output(prev, merged_outputs):\n",
    "    merged_outputs = sorted(merged_outputs.items(), key=lambda x: -x[1])\n",
    "    best_output, best_ratio = merged_outputs[0][0], merged_outputs[0][1] / merged_outputs[1][1]\n",
    "    dist_outputs = {}\n",
    "    for key, value in merged_outputs:\n",
    "        dist = distance(prev, key)\n",
    "        new_prob = value * 0.2 ** dist\n",
    "        dist_outputs[key] = new_prob\n",
    "    dist_outputs = sorted(dist_outputs.items(), key=lambda x: -x[1])\n",
    "    new_output, new_ratio = dist_outputs[0][0], dist_outputs[0][1] / dist_outputs[1][1]\n",
    "    if new_ratio > best_ratio:\n",
    "        best_output = new_output\n",
    "        best_ratio = new_ratio\n",
    "    return best_output, best_ratio\n",
    "    \n",
    "\n",
    "\n",
    "def autocorrect(text, line):\n",
    "    lines = [x + \"\\n\" for x in text.split(\"\\n\")]\n",
    "    curr_tokens, curr_token_strings, line_offset = tokenize(lines, line)\n",
    "\n",
    "    best_suggestion = 0\n",
    "    suggestions = []\n",
    "    for i in range(len(curr_tokens)):\n",
    "        prev = curr_token_strings[i]\n",
    "        curr_token_strings[i] = \"<mask>\"\n",
    "        string = \" \".join(curr_token_strings).strip()\n",
    "        curr_token_strings[i] = prev\n",
    "\n",
    "        outputs = fill_mask(string)\n",
    "        merged_outputs = merge_outputs(outputs)\n",
    "        if len(merged_outputs) < 2:\n",
    "            continue\n",
    "\n",
    "        best_output, best_ratio = get_best_output(prev, merged_outputs)\n",
    "        # print(i, \"\\t\", prev, \"\\t\", best_output, \"\\t\", best_ratio)\n",
    "        if best_output.strip() != prev.strip() and len(best_output.strip()) > 0 and best_ratio > 5:\n",
    "            print(\"CHANGE {\", prev, \"} to {\", best_output, \"} (\", best_ratio, \")\")\n",
    "            start = line_offset + curr_tokens[i].start[1]\n",
    "            end = line_offset + curr_tokens[i].end[1]\n",
    "            suggestions.append((((prev, start, end), best_output), best_ratio))\n",
    "            best_suggestion = max(best_suggestion, best_ratio)\n",
    "        \n",
    "    suggestions = [s[0] for s in suggestions if s[1] >= 0.5 * best_suggestion]\n",
    "    return suggestions\n",
    "\n",
    "\n",
    "text = \"\"\"print(\"Hello World\"] \"\"\"\n",
    "line = 0  # center of attention\n",
    "suggestions = autocorrect(text, line)\n",
    "\n",
    "print()\n",
    "print(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68998e60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2869a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
