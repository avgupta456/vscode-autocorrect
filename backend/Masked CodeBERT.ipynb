{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8276324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neulab/codebert-python\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"neulab/codebert-python\")\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d86dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9995896220207214, 'token': 41975, 'token_str': 'import', 'sequence': 'import numpy as np'}\n",
      "{'score': 0.0001533473696326837, 'token': 7761, 'token_str': 'from', 'sequence': 'from numpy as np'}\n",
      "{'score': 0.00015234711463563144, 'token': 6595, 'token_str': ' import', 'sequence': ' import numpy as np'}\n",
      "{'score': 3.692310565384105e-05, 'token': 10431, 'token_str': '#', 'sequence': '# numpy as np'}\n",
      "{'score': 1.0732301234384067e-05, 'token': 41929, 'token_str': 'Import', 'sequence': 'Import numpy as np'}\n",
      "{'score': 8.840012924338225e-06, 'token': 20891, 'token_str': ' Import', 'sequence': ' Import numpy as np'}\n",
      "{'score': 5.5008940762490965e-06, 'token': 6, 'token_str': ',', 'sequence': ', numpy as np'}\n",
      "{'score': 4.772637112182565e-06, 'token': 46181, 'token_str': 'package', 'sequence': 'package numpy as np'}\n",
      "{'score': 2.338359308851068e-06, 'token': 50118, 'token_str': '\\n', 'sequence': '\\n numpy as np'}\n",
      "{'score': 2.3127922759158537e-06, 'token': 1437, 'token_str': ' ', 'sequence': '  numpy as np'}\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(\"<mask> numpy as np\", top_k=10)\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a2cba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.875249445438385, 'token': 8, 'token_str': ' and', 'sequence': 'if (x is not None) and (x > 0)'}\n",
      "{'score': 0.017183667048811913, 'token': 50, 'token_str': ' or', 'sequence': 'if (x is not None) or (x > 0)'}\n",
      "{'score': 0.013177888467907906, 'token': 463, 'token_str': 'and', 'sequence': 'if (x is not None)and (x > 0)'}\n",
      "{'score': 0.012697670608758926, 'token': 671, 'token_str': ' return', 'sequence': 'if (x is not None) return (x > 0)'}\n",
      "{'score': 0.010224265046417713, 'token': 48200, 'token_str': ' &&', 'sequence': 'if (x is not None) && (x > 0)'}\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(\"if (x is not None) <mask> (x > 0)\")\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83439bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9669309854507446, 'token': 16, 'token_str': ' is', 'sequence': '<s>if var1 is<mask> None:</s>'}\n",
      "{'score': 0.00770614156499505, 'token': 328, 'token_str': '!', 'sequence': '<s>if var1!<mask> None:</s>'}\n",
      "{'score': 0.002733553759753704, 'token': 28696, 'token_str': ' <', 'sequence': '<s>if var1 <<mask> None:</s>'}\n",
      "{'score': 0.002088442211970687, 'token': 50118, 'token_str': '\\n', 'sequence': '<s>if var1\\n<mask> None:</s>'}\n",
      "{'score': 0.0018904786556959152, 'token': 35, 'token_str': ':', 'sequence': '<s>if var1:<mask> None:</s>'}\n",
      "\n",
      "{'score': 0.9925784468650818, 'token': 45, 'token_str': ' not', 'sequence': '<s>if var1<mask> not None:</s>'}\n",
      "{'score': 0.001338448142632842, 'token': 5214, 'token_str': '=', 'sequence': '<s>if var1<mask>= None:</s>'}\n",
      "{'score': 0.0013240614207461476, 'token': 16, 'token_str': ' is', 'sequence': '<s>if var1<mask> is None:</s>'}\n",
      "{'score': 0.0009433833765797317, 'token': 49333, 'token_str': '!=', 'sequence': '<s>if var1<mask>!= None:</s>'}\n",
      "{'score': 0.0006681906525045633, 'token': 45994, 'token_str': ' ==', 'sequence': '<s>if var1<mask> == None:</s>'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(\"if var1 <mask> <mask> None:\")\n",
    "for output in outputs:\n",
    "    for sub_output in output:\n",
    "        print(sub_output)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27dd0587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.1390775889158249, 'token': 17265, 'token_str': 'print', 'sequence': 'print ( x )'}\n",
      "{'score': 0.09198562055826187, 'token': 5780, 'token_str': ' print', 'sequence': ' print ( x )'}\n",
      "{'score': 0.03168381378054619, 'token': 41975, 'token_str': 'import', 'sequence': 'import ( x )'}\n",
      "{'score': 0.01466455589979887, 'token': 1423, 'token_str': ' y', 'sequence': ' y ( x )'}\n",
      "{'score': 0.013982338830828667, 'token': 37131, 'token_str': ' eval', 'sequence': ' eval ( x )'}\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(\"<mask> ( x )\")\n",
    "for output in outputs:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6b6d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ickey\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"./vocab.json\", encoding=\"utf-8\") as fp:\n",
    "      current_content = fp.read()\n",
    "        \n",
    "words = [k for k in json.loads(current_content).keys() if k.isalnum() and k.isascii() and k[0].isalpha() and len(k) > 3 and len(k) < 7 and k.islower()]\n",
    "\n",
    "_keywords = [\"and\", \"as\", \"assert\", \"break\", \"class\", \"continue\", \"def\", \"del\", \"elif\", \"else\", \"except\", \"False\", \"finally\", \"for\", \"from\", \"global\", \"if\", \"import\", \"in\", \"is\", \"lambda\", \"None\", \"nonlocal\", \"not\", \"or\", \"pass\", \"raise\", \"return\", \"True\", \"try\", \"while\", \"with\", \"yield\"]\n",
    "_builtins = [\"abs\", \"aiter\", \"all\", \"any\", \"anext\", \"ascii\", \"bin\", \"bool\", \"breakpoint\", \"bytearray\", \"bytes\", \"callable\", \"chr\", \"classmethod\", \"compile\", \"complex\", \"delattr\", \"dict\", \"dir\", \"divmod\", \"enumerate\", \"eval\", \"exec\", \"filter\", \"float\", \"format\", \"frozenset\", \"getattr\", \"globals\", \"hasattr\", \"hash\", \"help\", \"hex\", \"id\", \"input\", \"int\", \"isinstance\", \"issubclass\", \"iter\", \"len\", \"list\", \"locals\", \"map\", \"max\", \"memoryview\", \"min\", \"next\", \"object\", \"oct\", \"open\", \"ord\", \"pow\", \"print\", \"property\", \"range\", \"repr\", \"reversed\", \"round\", \"set\", \"setattr\", \"slice\", \"sorted\", \"staticmethod\", \"str\", \"sum\", \"super\", \"tuple\", \"type\", \"vars\", \"zip\", \"__import__\"]\n",
    "keywords = _keywords + _builtins\n",
    "\n",
    "words = [w for w in words if w not in keywords]\n",
    "\n",
    "print(random.choice(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08ae37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name = \"Al\"', 'x = 0', 'print ( \"My name is\" + fooba )', '']\n",
      "['rison = \"Al\"', 'cycle = 0', 'print ( \"My name is\" + oret )', '']\n",
      "0 \t print \t  \t 1.5701437182716753\n",
      "1 \t ( \t ( \t 21981.990061439225\n",
      "3 \t + \t + \t 8.07079024512817\n",
      "4 \t oret \t name \t 14.610650468306705\n",
      "CHANGE { oret } to { name } ( 14.610650468306705 )\n",
      "5 \t ) \t ) \t 217.0673353451805\n",
      "\n",
      "[(('fooba', 39, 44), 'name')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from tokenize import generate_tokens\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "def merge_outputs(outputs):\n",
    "    probs = defaultdict(float)\n",
    "    for output in outputs:\n",
    "        probs[output[\"token_str\"].strip()] += output[\"score\"]\n",
    "    return probs\n",
    "\n",
    "\n",
    "def tokenize(lines):\n",
    "    mapping = {}\n",
    "    inv_mapping = {}\n",
    "    N = len(lines)\n",
    "    tokens = list(generate_tokens(lambda L=iter(lines): next(L)))\n",
    "    filtered_tokens = defaultdict(list)\n",
    "    filtered_strings = defaultdict(list)\n",
    "    for token in tokens:\n",
    "        _line = token.start[0] - 1\n",
    "        filtered_tokens[_line].append(token)\n",
    "        if token.type == 1 and token.string not in keywords:\n",
    "            if token.string not in mapping:\n",
    "                mapping[token.string] = random.choice(words)\n",
    "                inv_mapping[mapping[token.string]] = token.string\n",
    "            filtered_strings[_line].append(mapping[token.string])\n",
    "        else:\n",
    "            filtered_strings[_line].append(token.string)\n",
    "    line_lengths = [filtered_tokens[i][-1].end[1] for i in range(N)]\n",
    "    cumulative_lengths = [sum(line_lengths[:i]) for i in range(N)]\n",
    "    \n",
    "    return inv_mapping, filtered_tokens, filtered_strings, cumulative_lengths\n",
    "\n",
    "\n",
    "def get_best_output(prev, merged_outputs):\n",
    "    merged_outputs = sorted(merged_outputs.items(), key=lambda x: -x[1])\n",
    "    best_output, best_ratio = merged_outputs[0][0], merged_outputs[0][1] / merged_outputs[1][1]\n",
    "    dist_outputs = {}\n",
    "    for key, value in merged_outputs:\n",
    "        dist = distance(prev, key)\n",
    "        new_prob = value * 0.2 ** dist\n",
    "        dist_outputs[key] = new_prob\n",
    "    dist_outputs = sorted(dist_outputs.items(), key=lambda x: -x[1])\n",
    "    new_output, new_ratio = dist_outputs[0][0], dist_outputs[0][1] / dist_outputs[1][1]\n",
    "    if new_ratio > best_ratio:\n",
    "        best_output = new_output\n",
    "        best_ratio = new_ratio\n",
    "    return best_output, best_ratio\n",
    "    \n",
    "\n",
    "\n",
    "def autocorrect(text, line, context=True):\n",
    "    lines = [x + \"\\n\" for x in text.split(\"\\n\")]\n",
    "    \n",
    "    inv_mapping, filtered_tokens, filtered_strings, cumulative_lengths = tokenize(lines)\n",
    "    \n",
    "    curr_tokens = filtered_tokens[line]\n",
    "    curr_token_strings = filtered_strings[line]\n",
    "    line_offset = cumulative_lengths[line]\n",
    "    \n",
    "    N = len(filtered_strings)\n",
    "    unprocessed_lines = [\" \".join(token.string for token in filtered_tokens[i]).strip() for i in range(N)]\n",
    "    processed_lines = [\" \".join(filtered_strings[i]).strip() for i in range(N)]\n",
    "    print(unprocessed_lines)\n",
    "    print(processed_lines)\n",
    "\n",
    "    best_suggestion = 0\n",
    "    suggestions = []\n",
    "    prev_lines = \"\\n\".join([processed_lines[i] for i in range(line-2, line)]) if context else \"\"\n",
    "    next_lines = \"\\n\".join(lines[line+1:line+3]) if context else \"\"\n",
    "    for i in range(len(curr_tokens)):\n",
    "        prev = curr_token_strings[i]\n",
    "        if len(prev.strip()) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Don't modify strings\n",
    "        if curr_tokens[i].type == 3:\n",
    "            continue\n",
    "        \n",
    "        curr_token_strings[i] = \"<mask>\"\n",
    "        string = \" \".join(curr_token_strings).strip()\n",
    "        curr_token_strings[i] = prev\n",
    "\n",
    "        outputs = fill_mask(prev_lines + string + next_lines)\n",
    "        merged_outputs = merge_outputs(outputs)\n",
    "        if len(merged_outputs) < 2:\n",
    "            continue\n",
    "\n",
    "        best_output, best_ratio = get_best_output(prev, merged_outputs)\n",
    "        print(i, \"\\t\", prev, \"\\t\", best_output, \"\\t\", best_ratio)\n",
    "        if best_output.strip() != prev.strip() and len(best_output.strip()) > 0 and best_ratio > 5:\n",
    "            print(\"CHANGE {\", prev, \"} to {\", best_output, \"} (\", best_ratio, \")\")\n",
    "            _from = prev\n",
    "            if _from in inv_mapping:\n",
    "                _from = inv_mapping[prev]\n",
    "            start = line_offset + curr_tokens[i].start[1]\n",
    "            end = line_offset + curr_tokens[i].end[1]\n",
    "            _to = best_output\n",
    "            if _to in inv_mapping:\n",
    "                _to = inv_mapping[best_output]\n",
    "            suggestions.append((((_from, start, end), _to), best_ratio))\n",
    "            best_suggestion = max(best_suggestion, best_ratio)\n",
    "        \n",
    "    suggestions = [s[0] for s in suggestions if s[1] >= 0.5 * best_suggestion]\n",
    "    return suggestions\n",
    "\n",
    "\n",
    "text = \"\"\"name = \"Al\"\n",
    "x = 0\n",
    "print(\"My name is\" + fooba)\"\"\"\n",
    "line = 2  # center of attention\n",
    "suggestions = autocorrect(text, line, context=True)\n",
    "\n",
    "print()\n",
    "print(suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31856d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
